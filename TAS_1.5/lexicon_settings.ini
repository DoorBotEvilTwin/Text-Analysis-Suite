# Configuration for Text Analysis Suite Modules

[General]
# Choose the interpretation mode:
# rules      = Use built-in simple rule-based interpretation.
# local_llm  = Use a local LLM backend (like KoboldCpp).
# api = Use external LLM API. Disabled for now.
# mode = rules

[LocalLLM]
# Settings for using a local LLM backend (like KoboldCpp)
enabled = true

# The full URL to your running KoboldCpp API endpoint (usually /api/v1)
# Ensure KoboldCpp is running BEFORE starting this script if enabled=true.
api_base = http://localhost:5001/v1/chat/completions

# Optional: Model name (often ignored by KoboldCpp if model is preloaded, but good for logging)
# https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF # Best for uncensored
# https://huggingface.co/Mungert/Qwen2.5-VL-7B-Instruct-GGUF # Best for speed
transformer_model = gemma-3-27b-it-abliterated.q8_0
processor_model = gemma-3-27b-it-abliterated.q8_0

# Optional: Generation parameters
temperature = 0.0
max_tokens = 256
# top_p = 0.9
# top_k = 40
# --- Add other local LLM providers similarly if they offer compatible endpoints ---

### Developer Notes

## Fentible: I noticed the 4B failed certain image tasks the 12B and 27B could do. Is this technicality documented anywhere?
## Gemini: The jump in multimodal performance from 4B to 12B is more than twice as big as the positive performance jump from 12B to 27B based on this benchmark data. In fact, the average absolute improvement is more than double (10.7 vs 4.5), and the average percentage improvement is nearly triple (18.4% vs 6.5%). This suggests that while scaling up to 27B does offer some advantages on specific challenging tasks, the 12B model represents a more substantial leap in overall multimodal understanding and reasoning compared to the 4B model. https://ai.google.dev/gemma/docs/core/model_card_3
## Ranking (Quality): Gemma (27B) remains slightly preferred for conciseness/potential nuance. Gemma (12B) is very capable but more verbose. Gemma (4B) fails.
## Speed: Gemma (12B) is significantly faster (~2.2x) than Gemma (27B).
## Recommendation: Since Gemma (12B) successfully completed all tasks (including the heatmap) AND is over 2x faster, use Gemma (12B). The substantial speed advantage justifies accepting its slightly more verbose style over the slower Gemma (27B).
## Gemma (27B) vs. Gemma (12B): Subtle difference (~5-15%). Mainly stylistic (conciseness).
## Gemma (12B) vs. Gemma (4B): Massive difference (~70-90%+). Functional failure vs. success; accuracy issues.

## Okay, here is the comparison chart consolidating the analysis of Ben, Beth, and Leo, followed by a summary and comparison to the provided reports.
## **Model Performance Comparison Chart**
## 
## | Feature / Dimension                     | Leo (Gemma: 4B)                     | Beth (Gemma: 12B)                   | Ben (Gemma: 27B)                     |
## | :-------------------------------------- | :---------------------------------- | :---------------------------------- | :----------------------------------- |
## | **Heatmap Dendrogram Task (#9)**        | **FAIL** (Asked for image)          | **PASS** (Generated list)           | **PASS** (Generated list)            |
## | **Accuracy (Other Plots)**              | Poor (Wrong acronyms)               | Good                                | Excellent                            |
## | **Accuracy (File Summaries - Numbers)** | Poor (Multiple factual errors)      | Good (Generally accurate)           | Excellent (Accurate comparisons)     |
## | **Accuracy (Augmented Analysis)**       | Poor (Hallucinated correlations)    | Good (Reasonable correlations)      | Excellent (Specific visual details)  |
## | **Quality/Nuance/Insight**              | Low (Generic, inaccurate)           | Good (Competent, less detailed)     | Excellent (Detailed, insightful)     |
## | **Conciseness**                         | Poor (Errors make it irrelevant)    | Good (Slightly verbose at times)    | Excellent (Direct, informative)      |
## | **Benchmark Gen. Speed (T/s)**          | **49.50 (Fastest)**                 | 2.49 (Slow)                         | 1.12 (Slowest)                       |
## | **Overall Rank (Quality)**              | 3rd (Unusable for complex tasks)    | 2nd (Good, viable alternative)      | **1st (Highest Quality)**            |
## | **Overall Rank (Speed)**                | **1st**                             | 2nd                                 | 3rd                                  |
## 
## **Consolidated Summary & Comparison**
## 
## *   **Model Identification:** The analysis strongly suggests Leo is the 4B model (95% certainty), Beth is the 12B (85% certainty), and Ben is the 27B (85% certainty).
## *   **Key Differentiator:** The most critical difference is the failure of Leo (4B) on the complex multimodal heatmap dendrogram analysis task, where it couldn't interpret the image structure. Both Beth (12B) and Ben (27B) successfully completed this task. Leo also exhibited significant factual hallucinations when comparing numerical values in file summaries and misinterpreting metric acronyms, rendering it unreliable for this suite.
## *   **Quality Hierarchy:** There's a clear quality progression: Ben (27B) > Beth (12B) > Leo (4B).
##     *   **Ben (27B):** Provides the most accurate, detailed, insightful, and concise responses across all tasks, excelling at the complex augmented analysis requiring synthesis of numerical and visual data.
##     *   **Beth (12B):** Demonstrates solid competence, completing all tasks accurately but with slightly less detail, nuance, and visual correlation sharpness compared to Ben. Its summaries are good but occasionally contain more introductory filler.
##     *   **Leo (4B):** Fails critical tasks and shows poor accuracy and reasoning even on simpler numerical comparisons and plot summaries.
## *   **Speed vs. Quality:** The benchmark speeds confirm Leo (4B) is vastly faster, but its unreliability makes this irrelevant. Beth (12B) is significantly faster (>2x) than Ben (27B) at text generation. Given that Beth *successfully completed all tasks*, this speed advantage makes it a very strong contender, potentially justifying its use over Ben if the slightly lower (but still good) quality is acceptable for the user's needs. Ben offers the highest quality but at the slowest speed.
## *   **Comparison to AI Benchmark Report:** The observed performance aligns perfectly with the provided AI report comparing 4B, 12B, and 27B multimodal capabilities:
##     *   The **large jump** in capability reported between 4B and 12B is clearly reflected in Leo's failure vs. Beth's success on the complex heatmap task.
##     *   The **smaller, incremental improvement** reported between 12B and 27B is reflected in Beth's competence vs. Ben's higher refinement, detail, and synthesis ability. The 27B model shows superior quality, but the 12B model already possesses the core capability needed for the tasks in this suite.

## **Certainty Level of Leo (Excluding Heatmap Failure):**
## Even without considering the critical failure on the heatmap task, the evidence from the other summaries (misinterpreted acronyms, factual errors in simple comparisons, less accurate plot descriptions) is still quite strong. These are hallmarks of a smaller, less capable model struggling with domain-specific terms and basic numerical reasoning within a language context. Therefore, my certainty that **Leo is the 4B model**, based *only* on the first 8 plot summaries and the individual file summaries, would be **around 80-85%**. The heatmap failure was definitive proof, but the pattern of errors and lower quality in the other tasks provides significant supporting evidence on its own.