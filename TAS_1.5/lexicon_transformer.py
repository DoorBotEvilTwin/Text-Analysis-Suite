# lexicon_transformer.py
# (Includes numerical dendrogram analysis + LLM visual augmentation)
# (Loads plot metadata from JSON for individual plot summaries)
# (Adds LLM summaries for individual plots, inserted below each plot image)
# (Updates HTML incrementally after EACH LLM call or analysis step)
# (Performs clustered heatmap analysis last)
# PATCHED: Fixed NameError for PLOT_KEY_TO_FILENAME_BASE by passing plot_metadata
# PATCHED: Corrected image search logic in update_html_report to use 'alt' attribute
# PATCHED: Main dendrogram explanation uses x2 default max_tokens
# PATCHED: Added warning if LLM visual explanation lacks numbered items
# PATCHED: Added webbrowser auto-launch for HTML report
# PATCHED: Fixed imputation shape mismatch error in preprocess_metrics_for_analysis.
# PATCHED: Updated plot_order list to match keys from visualizer's metadata.
# PATCHED: Improved heatmap numbering logic using linkage matrix analysis.
# PATCHED: Updated dendrogram/augmented analysis prompts for improved numbering.
# PATCHED: Corrected data loading to respect separator index.
# PATCHED: Refined heatmap numbering placement (vertical and horizontal).
# PATCHED: Added missing 'import math'. # <<< THIS FIX
# PATCHED: Corrected data loading/filtering logic to handle collective metrics rows. # <<< THIS FIX

import os
import sys
import logging
import configparser
import json
import time
import base64
from io import BytesIO
import re # For parsing LLM response and HTML content
import pandas as pd
import numpy as np
import threading # Added for timed input
import webbrowser # Added for auto-launch
import math # <<< FIX: Added missing import

# --- Dependencies with Checks ---
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    requests = None
    REQUESTS_AVAILABLE = False
    logging.error("CRITICAL: 'requests' library not found (pip install requests). Local LLM calls are required and will fail.")

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    Image = None
    ImageDraw = None
    ImageFont = None
    PIL_AVAILABLE = False
    logging.error("CRITICAL: 'Pillow' library not found (pip install Pillow). Image modification is required and will fail.")

try:
    from bs4 import BeautifulSoup, NavigableString # Import NavigableString
    BS4_AVAILABLE = True
except ImportError:
    BeautifulSoup = None
    NavigableString = None # Define as None if bs4 missing
    BS4_AVAILABLE = False
    logging.error("CRITICAL: 'beautifulsoup4' library not found (pip install beautifulsoup4). HTML update is required and will fail.")

# --- Try importing lxml for potentially better parsing ---
try:
    import lxml
    LXML_AVAILABLE = True
    logging.debug("lxml parser is available.")
except ImportError:
    lxml = None
    LXML_AVAILABLE = False
    logging.debug("lxml parser not found, falling back to html.parser.")

# --- SciPy needed for linkage traversal ---
try:
    from scipy.cluster import hierarchy # Needed for dendrogram analysis
    SCIPY_AVAILABLE = True
except ImportError:
    hierarchy = None
    SCIPY_AVAILABLE = False
    logging.warning("SciPy not found. Numerical dendrogram analysis will be limited.")

# --- Sklearn for preprocessing ---
try:
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.impute import SimpleImputer
    SKLEARN_AVAILABLE = True
except ImportError:
    MinMaxScaler = None
    SimpleImputer = None
    SKLEARN_AVAILABLE = False
    logging.warning("scikit-learn not found. Preprocessing will use basic methods.")


# --- Configuration Files ---
INI_FILENAME = "lexicon_settings.ini" # Uses the same INI as the processor
HTML_FILENAME = "_LEXICON.html" # Target HTML file to modify
LOG_FILENAME = "_LEXICON_errors.log" # Appends to the same log
CSV_FILENAME = "_LEXICON.csv" # Source for metric data

# --- Files generated by Visualizer (MUST BE PRESENT) ---
HEATMAP_FILENAME = os.path.join("plots", "clustered_metrics_heatmap.png") # Relative path to input image
MODIFIED_HEATMAP_FILENAME = os.path.join("plots", "clustered_metrics_heatmap_numbered.png") # Relative path for output image
LINKAGE_MATRIX_FILE = "_linkage_matrix.npy" # Numerical dendrogram structure
REORDERED_INDICES_FILE = "_reordered_indices.npy" # Order of files in heatmap rows
ORIGINAL_FILENAMES_FILE = "_original_filenames.json" # Optional mapping index->filename
PLOT_METADATA_FILENAME = "_plot_metadata.json" # File containing metadata for generated plots

# --- AI/Local LLM Configuration ---
MAX_RETRIES = 2
RETRY_DELAY = 5
DEFAULT_MAX_TOKENS = 256
PLOT_SUMMARY_MAX_TOKENS_MULTIPLIER = 2.0 # Multiplier for individual plot summaries
DETAILED_ANALYSIS_MAX_TOKENS_MULTIPLIER = 4.0 # Multiplier for augmented analysis

# --- Analysis Configuration ---
# Metrics to focus on for numerical sub-branch comparison & LLM prompt context
KEY_METRICS_FOR_ANALYSIS = [
    'MTLD', 'VOCD', 'Average Sentence Length', 'Flesch Reading Ease',
    'Distinct-2', 'Repetition-2', 'Lexical Density', 'TTR'
]
NUM_SUB_BRANCH_LEVELS_TO_ANALYZE = 3 # Adjust as needed

# --- Image Numbering Configuration ---
MAX_LABELS_TO_PLACE = 6 # Keep at 6 for now based on previous testing
LABEL_FONT_SIZE_RATIO = 0.020 # Ratio of image height for font size
LABEL_COLOR = (255, 0, 0) # Red
LABEL_BG_COLOR = (255, 255, 255, 190) # Semi-transparent white
# --- PATCH: Define dendrogram area ratio for X positioning ---
DENDROGRAM_WIDTH_RATIO = 0.15 # Estimated width ratio of the row dendrogram area
LABEL_X_PADDING_RATIO = 0.01 # Padding from left edge

# --- Basic Logging Setup ---
log_path = os.path.join(os.path.dirname(__file__) if "__file__" in locals() else os.getcwd(), LOG_FILENAME)
log_level = logging.INFO # Change to logging.DEBUG for verbose analysis steps
# Remove existing handlers
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)
logging.basicConfig(
    level=log_level,
    format='%(asctime)s [%(levelname)s] %(filename)s:%(lineno)d - %(message)s',
    handlers=[
        logging.FileHandler(log_path, mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logging.info(f"\n{'='*10} Starting Dendrogram Transformer Run (with Numerical Analysis & Plot Summaries) {'='*10}")

# --- START: Added Helper Function for Timed Input ---
def get_input_with_timeout(prompt, timeout):
    """Gets input from the user with a timeout and countdown."""
    user_input = [None] # Use a list to allow modification from thread
    interrupted = [False] # Flag to signal if user provided input

    def _get_input_target(prompt_str, result_list, interrupt_flag):
        try:
            result_list[0] = input(prompt_str)
            interrupt_flag[0] = True # Signal that input was received
        except EOFError: # Handle case where input stream is closed unexpectedly
            pass

    thread = threading.Thread(target=_get_input_target, args=(prompt, user_input, interrupted))
    thread.daemon = True # Allow program to exit even if thread is stuck
    thread.start()

    # Countdown loop
    for i in range(timeout, 0, -1):
        if interrupted[0]: break
        print(f"\rNo path provided. Using current directory in {i}s... Press Enter or type path to override: ", end="", flush=True)
        time.sleep(1)
        if interrupted[0]: break
    print("\r" + " " * 100 + "\r", end="", flush=True) # Overwrite with spaces
    if interrupted[0] and user_input[0] is not None: return user_input[0]
    else: return None
# --- END: Added Helper Function ---


# --- Helper Functions ---

# --- Data Loading ---
def load_config(ini_path):
    config = configparser.ConfigParser()
    if not os.path.exists(ini_path):
        logging.critical(f"CRITICAL: Configuration file '{INI_FILENAME}' not found at '{ini_path}'.")
        return None
    try:
        config.read(ini_path)
        logging.info(f"Loaded configuration from '{INI_FILENAME}'.")
        return config
    except configparser.Error as e:
        logging.critical(f"CRITICAL: Error parsing configuration file '{INI_FILENAME}': {e}")
        return None

def get_llm_config(config):
    if not config or 'LocalLLM' not in config:
        logging.error("[LocalLLM] section not found in INI file. Cannot proceed.")
        return None, False
    local_llm_enabled = config.getboolean('LocalLLM', 'enabled', fallback=False)
    if not local_llm_enabled:
        logging.error("Local LLM mode is disabled in configuration ([LocalLLM] enabled = false). Cannot proceed.")
        return None, False
    local_llm_config = dict(config.items('LocalLLM'))
    if not local_llm_config.get('api_base'):
        logging.error("Local LLM mode enabled, but 'api_base' is not set in [LocalLLM] section. Cannot proceed.")
        return None, False
    if 'max_tokens' not in local_llm_config:
        local_llm_config['max_tokens'] = str(DEFAULT_MAX_TOKENS)
        logging.info(f"Setting default max_tokens to {DEFAULT_MAX_TOKENS}")
    logging.info(f"Local LLM mode is enabled. Config: {local_llm_config}")
    return local_llm_config, True

def find_separator_index(filename):
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if "--- Collective Metrics (E) ---" in line:
                    return i
    except FileNotFoundError: logging.error(f"CSV file not found at {filename} for separator check."); return None
    except Exception as e: logging.error(f"Error reading CSV to find separator: {e}"); return None
    logging.warning(f"Collective metrics separator not found in {filename}.")
    return None

# --- PATCHED: load_metric_data respects separator AND filters ---
def load_metric_data(csv_path):
    """Loads the main metrics data from the CSV file, stopping before collective metrics."""
    separator_idx = find_separator_index(csv_path)
    df = None
    try:
        if separator_idx is None:
            logging.warning(f"Could not find collective metrics separator in {csv_path}. Reading entire file.")
            df = pd.read_csv(csv_path)
        else:
            rows_to_read = separator_idx
            logging.info(f"Separator line found at index {separator_idx}. Reading {rows_to_read} data rows.")
            df = pd.read_csv(csv_path, nrows=rows_to_read)

        df.dropna(how='all', inplace=True) # Drop rows where ALL values are NaN

        # <<< FIX: Explicitly filter known collective keys AFTER loading nrows >>>
        # (Need to parse collective keys first to know what to filter)
        # This is slightly inefficient but ensures we have the keys
        temp_collective_metrics = {}
        if separator_idx is not None:
            try:
                with open(csv_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    start_line = separator_idx + 1
                    if start_line < len(lines):
                        if not lines[start_line].strip(): start_line += 1
                        for line in lines[start_line:]:
                            line = line.strip()
                            if not line: continue
                            match = re.match(r'^"?([^"]+)"?,\s*(.*)$', line)
                            if match: temp_collective_metrics[match.group(1).strip()] = True # Just need the keys
            except Exception: pass # Ignore errors here, just trying to get keys

        known_collective_keys = list(temp_collective_metrics.keys()) + ["--- Collective Metrics (E) ---"]
        if 'Filename' in df.columns:
            initial_rows = len(df)
            df = df[~df['Filename'].isin(known_collective_keys)].copy()
            if len(df) < initial_rows:
                logging.info(f"Filtered out {initial_rows - len(df)} collective metric rows from main data section.")
        # <<< END FIX >>>

        logging.info(f"Loaded {df.shape[0]} data rows from CSV for metric analysis.")
        # Ensure Filename column exists, use index otherwise
        if 'Filename' not in df.columns:
             logging.warning("CSV has no 'Filename' column. Using index.")
             df['Filename'] = [f"File Index {i}" for i in df.index]
        return df
    except FileNotFoundError:
        logging.error(f"CSV file '{csv_path}' not found.")
        return None
    except Exception as e:
        logging.error(f"Could not read CSV file: {e}", exc_info=True)
        return None

def load_npy_file(npy_path):
    """Loads a NumPy .npy file."""
    if not os.path.exists(npy_path):
        logging.error(f"Required NPY file not found: {npy_path}. Ensure visualizer saves it.")
        return None
    try:
        data = np.load(npy_path)
        logging.info(f"Successfully loaded {os.path.basename(npy_path)}.")
        return data
    except Exception as e:
        logging.error(f"Failed to load NPY file {npy_path}: {e}", exc_info=True)
        return None

def load_json_file(json_path):
    """Loads a JSON file."""
    if not os.path.exists(json_path):
        logging.warning(f"Optional JSON file not found: {json_path}.")
        return None
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logging.info(f"Successfully loaded {os.path.basename(json_path)}.")
        return data
    except Exception as e:
        logging.error(f"Failed to load JSON file {json_path}: {e}", exc_info=True)
        return None

# --- Data Preprocessing ---
def preprocess_metrics_for_analysis(df_metrics):
    """Prepares metric data for numerical analysis (impute, maybe normalize)."""
    if df_metrics is None: return None, None

    df_numeric = df_metrics.copy()
    numeric_cols = [col for col in df_metrics.columns if col != 'Filename']

    # Convert to numeric, coercing errors and handling placeholders
    for col in numeric_cols:
        df_numeric[col] = df_numeric[col].replace(['FAIL', 'N/A (<=1 file)'], np.nan)
        df_numeric[col] = df_numeric[col].replace(['INF'], np.inf)
        df_numeric[col] = df_numeric[col].replace(['NaN'], np.nan)
        df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')

    # Select only numeric columns for processing
    df_numeric_only = df_numeric.select_dtypes(include=np.number)
    if df_numeric_only.empty:
        logging.error("No numeric columns found after attempting conversion.")
        return None, None

    # Impute missing values (using mean)
    df_imputed = df_numeric_only.copy()
    imputed_columns = df_imputed.columns # Store original numeric columns

    if df_imputed.isnull().any().any():
        logging.info("Imputing missing values using column means...")
        if SimpleImputer and SKLEARN_AVAILABLE:
            imputer = SimpleImputer(strategy='mean')
            logging.debug(f"Columns before imputation: {df_imputed.columns.tolist()}")
            imputed_values = imputer.fit_transform(df_imputed)
            imputed_columns = imputer.get_feature_names_out(df_imputed.columns)
            logging.debug(f"Columns after imputation: {imputed_columns.tolist()}")
            df_imputed = pd.DataFrame(imputed_values, index=df_imputed.index, columns=imputed_columns)
        else:
            logging.warning("SimpleImputer not available. Filling NaNs with column mean manually.")
            df_imputed = df_imputed.fillna(df_imputed.mean())
            imputed_columns = df_imputed.columns # Update columns after fillna

        # Fill any remaining NaNs (e.g., all-NaN columns) with 0
        if df_imputed.isnull().any().any():
             logging.warning("NaNs remain after mean imputation. Filling with 0.")
             df_imputed.fillna(0, inplace=True)

    # Normalize data (0-1 scaling) - useful for comparing metrics later
    df_normalized = df_imputed.copy()
    if MinMaxScaler and SKLEARN_AVAILABLE:
        scaler = MinMaxScaler()
        scaled_values = scaler.fit_transform(df_normalized) # Use the imputed data
        df_normalized = pd.DataFrame(scaled_values, index=df_normalized.index, columns=imputed_columns) # Use imputed_columns
    else: # Manual fallback
        logging.warning("MinMaxScaler not available. Normalizing manually (may differ slightly).")
        for col in imputed_columns: # Use imputed_columns
            min_val = df_normalized[col].min(); max_val = df_normalized[col].max()
            if pd.notna(min_val) and pd.notna(max_val) and (max_val - min_val != 0):
                df_normalized[col] = (df_normalized[col] - min_val) / (max_val - min_val)
            else: df_normalized[col] = 0.5 # Assign mid-value if no variance or all NaN

    logging.info(f"Metrics preprocessed: Imputed NaNs, created normalized version.")
    # Return both imputed (original scale) and normalized
    return df_imputed, df_normalized


# --- Image/LLM Helpers ---
def encode_image_to_base64(image_path):
    if not os.path.exists(image_path):
        logging.error(f"Image file not found: {image_path}")
        return None
    try:
        with open(image_path, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
        logging.info(f"Successfully encoded image: {image_path}")
        return f"data:image/png;base64,{encoded_string}"
    except Exception as e:
        logging.error(f"Failed to encode image {image_path}: {e}", exc_info=True)
        return None

def call_local_llm_api_multimodal(prompt, base64_image_data, llm_config, max_tokens_override=None):
    if not REQUESTS_AVAILABLE or requests is None:
        return "Error: 'requests' library not installed (pip install requests)."
    if not base64_image_data:
        return "Error: No base64 image data provided."
    api_base = llm_config.get('api_base')
    if not api_base:
        return "Error: 'api_base' URL not configured in [LocalLLM] section of INI file."
    current_max_tokens = max_tokens_override if max_tokens_override is not None else int(llm_config.get('max_tokens', DEFAULT_MAX_TOKENS))
    payload = {
        "model": llm_config.get('transformer_model', llm_config.get('model', 'local-multimodal-model')), # Prioritize transformer_model
        "messages": [
            { "role": "user", "content": [ {"type": "text", "text": prompt}, { "type": "image_url", "image_url": { "url": base64_image_data } } ] }
        ],
        "temperature": float(llm_config.get('temperature', 0.0)),
        "max_tokens": current_max_tokens,
    }
    payload = {k: v for k, v in payload.items() if v is not None}
    endpoint = api_base
    logging.debug(f"Attempting local multimodal LLM call to: {endpoint}")
    logging.debug(f"Payload (excluding image data): {json.dumps({k: v for k, v in payload.items() if k != 'messages'}, indent=2)}")
    logging.debug(f"Prompt text (first 200 chars): {prompt[:200]}...")
    logging.debug(f"Max Tokens for this call: {current_max_tokens}")
    last_error = None
    for attempt in range(MAX_RETRIES):
        response = None
        try:
            logging.info(f"Sending request to LLM (Attempt {attempt+1}/{MAX_RETRIES}) with NO client-side timeout...")
            response = requests.post(endpoint, json=payload)
            logging.debug(f"Local LLM request sent. Status Code: {response.status_code if response else 'N/A'}")
            if response is None: raise requests.exceptions.RequestException("Request returned None unexpectedly.")
            response.raise_for_status()
            raw_response_text = response.text
            logging.debug(f"Raw response text received: {raw_response_text[:500]}...")
            response_json = response.json()
            logging.debug(f"Local LLM Response JSON: {json.dumps(response_json, indent=2)}")
            if 'choices' in response_json and len(response_json['choices']) > 0:
                 message = response_json['choices'][0].get('message', {})
                 content = message.get('content')
                 if content:
                      logging.info("Local LLM call successful.")
                      return content.strip()
                 else: last_error = "Error: 'content' field missing in response message."; logging.warning(last_error)
            else: last_error = "Error: Expected 'choices' array missing or empty in response."; logging.warning(last_error)
        except requests.exceptions.ConnectionError as e: last_error = f"Error: Connection failed to {endpoint} ({e}). Is the backend running and multimodal-capable?"; logging.error(last_error); break
        except requests.exceptions.RequestException as e: response_text_on_error = response.text[:500] if response is not None else "N/A"; status_code = response.status_code if response is not None else "N/A"; last_error = f"Error: Request failed (Status: {status_code}, Error: {e}). Response: {response_text_on_error}"; logging.error(f"Local LLM request failed (Attempt {attempt + 1}/{MAX_RETRIES}): {last_error}")
        except json.JSONDecodeError as e: raw_text_on_decode_error = response.text if response is not None else "N/A"; last_error = f"Error: Failed to decode JSON response from {endpoint}."; logging.error(f"{last_error} Raw Response: '{raw_text_on_decode_error}'", exc_info=False); break
        except Exception as e: last_error = f"Error: Unexpected error during local LLM call ({type(e).__name__}: {e})"; logging.error(last_error, exc_info=True); break
        if attempt < MAX_RETRIES - 1: logging.info(f"Retrying in {RETRY_DELAY}s..."); time.sleep(RETRY_DELAY)
        else: logging.error(f"LLM call failed after {MAX_RETRIES} attempts.")
    return last_error if last_error else "Error: LLM call failed after multiple retries."

def generate_dendrogram_interpretation_prompt(num_labels_to_generate=MAX_LABELS_TO_PLACE):
    """Generates the prompt for the main dendrogram visual interpretation."""
    prompt = f"""
Analyze the provided clustered heatmap image, focusing specifically on the **row dendrogram** (the tree-like structure on the left showing how rows/files are clustered).
Your task is to identify the most significant splits or branches in this row dendrogram and explain the inferred relationship or grouping logic they represent.
Output *only* a **numbered list**, starting with #1, describing the inferred meaning for each major split or branch you identify. Describe them in a logical order, typically starting from the largest groupings (splits further to the left, higher up the dendrogram) and moving to more specific sub-groups (splits further to the right, lower down). Aim for **exactly {num_labels_to_generate} points**, focusing on the most important structural divisions.
Example format:
#1: The primary division separates the files into two main groups: those with generally [Characteristic A based on heatmap patterns] versus those with [Characteristic B].
#2: Within the first main group (#1), a notable sub-division occurs, separating files exhibiting [Sub-characteristic A1] from those with [Sub-characteristic A2].
#3: The second main group (#1) appears more cohesive, but a minor split distinguishes a small outlier set based on [Characteristic C].
... (continue for other major branches identified, up to {num_labels_to_generate})
Be concise. Focus on the *relationships* and *grouping logic* revealed by the dendrogram structure. Do **not** describe the heatmap colors or specific metric names unless essential to explain the grouping logic. The numbering in your response is critical as it will correspond to labels placed on the image.
"""
    return prompt

def generate_plot_summary_prompt(plot_title, plot_desc):
    """Generates a prompt asking the LLM to summarize a given plot image."""
    prompt = f"""
You are provided with an image of a data visualization plot.

**Plot Title:** {plot_title}
**Plot Description:** {plot_desc}

Analyze the provided plot image and generate a concise summary (2-4 sentences) of the key findings or patterns visible in the plot. Focus on the main trends, distributions, correlations, or groupings shown. What is the primary message conveyed by this visualization based *only* on what you can see?
"""
    return prompt


def count_numbered_items(text):
    if not text or text.startswith("Error:"): return 0
    matches = re.findall(r"^\s*(?:#\s?(\d+)\s?[:.]?|(\d+)\.\s+)", text, re.MULTILINE)
    numbers = set()
    for m in matches:
        num_str = m[0] if m[0] else m[1]
        if num_str:
            try: numbers.add(int(num_str))
            except ValueError: continue
    count = len(numbers)
    logging.info(f"Found {count} numbered items in LLM response.")
    return count

# --- UPDATED Image Numbering Function ---
def add_numbers_to_image(image_path, output_path, num_labels, linkage_matrix, reordered_indices):
    """Adds numbered labels to the dendrogram based on linkage matrix analysis."""
    if not PIL_AVAILABLE: logging.error("Cannot add numbers to image: Pillow library not available."); return False
    if not os.path.exists(image_path): logging.error(f"Input image not found for numbering: {image_path}"); return False
    if num_labels <= 0:
        logging.warning("Number of labels to add is zero or less. Skipping numbering.")
        try: import shutil; shutil.copy2(image_path, output_path); logging.info(f"Copied original image to {output_path} as no labels were requested."); return True
        except Exception as e: logging.error(f"Failed to copy original image: {e}"); return False
    if linkage_matrix is None or reordered_indices is None or not SCIPY_AVAILABLE:
        logging.error("Linkage matrix, reordered indices, or SciPy not available. Cannot perform analysis for numbering. Skipping.")
        try: import shutil; shutil.copy2(image_path, output_path); logging.info(f"Copied original image to {output_path} as analysis data was missing."); return True
        except Exception as e: logging.error(f"Failed to copy original image: {e}"); return False

    actual_num_labels = min(num_labels, MAX_LABELS_TO_PLACE, len(linkage_matrix)) # Limit by available splits too
    if actual_num_labels < num_labels: logging.warning(f"Requested {num_labels} labels, but limited to {actual_num_labels} by available splits or max placement.")

    try:
        img = Image.open(image_path).convert("RGBA"); draw = ImageDraw.Draw(img); img_width, img_height = img.size
        font_size = max(12, int(img_height * LABEL_FONT_SIZE_RATIO)) # Smaller base size
        try: font = ImageFont.truetype("arial.ttf", font_size); logging.debug(f"Using Arial font size {font_size}")
        except IOError:
            logging.warning("Arial font not found. Using default PIL font.");
            try: font = ImageFont.load_default(size=font_size)
            except AttributeError: font = ImageFont.load_default()
            except Exception as e: logging.error(f"Could not load any default font: {e}"); font = None
        if font is None:
            logging.error("Font loading failed. Cannot draw text on image.")
            try: img.convert("RGB").save(output_path, "PNG"); logging.info(f"Saved original image to {output_path} due to font loading failure.")
            except Exception as save_err: logging.error(f"Failed to save original image after font error: {save_err}")
            return False

        # --- Dendrogram Analysis for Placement ---
        num_leaves = len(reordered_indices)
        # Sort linkage by distance (column 2) descending to find major splits
        sorted_linkage_indices = np.argsort(linkage_matrix[:, 2])[::-1]
        max_distance = linkage_matrix[sorted_linkage_indices[0], 2] if len(sorted_linkage_indices) > 0 else 1.0

        # Map original leaf index to its vertical position (0=top, 1=bottom) in the reordered dendrogram
        leaf_positions = {original_index: pos / (num_leaves - 1) if num_leaves > 1 else 0.5
                          for pos, original_index in enumerate(reordered_indices)}

        label_positions = {} # Store {label_num: (x_ratio, y_ratio)}

        # Analyze the top splits to determine label positions
        analyzed_nodes = set()
        labels_placed = 0
        for i in range(len(sorted_linkage_indices)):
            if labels_placed >= actual_num_labels: break

            linkage_idx = sorted_linkage_indices[i]
            node_id = num_leaves + linkage_idx
            distance = linkage_matrix[linkage_idx, 2]

            # Avoid labeling sub-splits if a parent split was already labeled
            left_child_id = int(linkage_matrix[linkage_idx, 0])
            right_child_id = int(linkage_matrix[linkage_idx, 1])
            if left_child_id in analyzed_nodes or right_child_id in analyzed_nodes:
                continue

            # Find leaves under this node
            current_leaves = get_cluster_leaves_recursive(linkage_matrix, node_id, num_leaves)
            if not current_leaves: continue

            # Calculate the vertical center of these leaves in the reordered plot
            positions = [leaf_positions.get(leaf_idx, 0.5) for leaf_idx in current_leaves]
            if not positions: continue
            avg_y_ratio = np.mean(positions)

            # --- Calculate Horizontal Position based on distance ---
            norm_dist = distance / max_distance if max_distance > 0 else 0
            # Map normalized distance to X ratio within the dendrogram area
            # Further distance (closer to 1) should be further left (closer to 0)
            # Closer distance (closer to 0) should be further right (closer to DENDROGRAM_WIDTH_RATIO)
            # Use a non-linear mapping (e.g., sqrt) to space out labels more towards the left
            x_ratio = LABEL_X_PADDING_RATIO + (1.0 - math.sqrt(norm_dist)) * (DENDROGRAM_WIDTH_RATIO - 2 * LABEL_X_PADDING_RATIO) # <<< FIX: Use math.sqrt
            x_ratio = max(LABEL_X_PADDING_RATIO, min(x_ratio, DENDROGRAM_WIDTH_RATIO - LABEL_X_PADDING_RATIO)) # Clamp within bounds

            labels_placed += 1
            label_positions[labels_placed] = (x_ratio, avg_y_ratio)

            # Mark children as analyzed to prioritize higher-level splits
            analyzed_nodes.add(left_child_id)
            analyzed_nodes.add(right_child_id)

        logging.info(f"Calculated {len(label_positions)} label positions based on dendrogram structure.")

        # --- Draw Labels ---
        for label_num, (x_ratio, y_ratio) in sorted(label_positions.items()):
            number_text = f"#{label_num}";
            x_pos = int(img_width * x_ratio)
            y_pos = int(img_height * y_ratio)

            try:
                # Calculate text bounding box for background
                if hasattr(draw, 'textbbox'): bbox = draw.textbbox((x_pos, y_pos), number_text, font=font, anchor="lm"); text_width = bbox[2] - bbox[0]; text_height = bbox[3] - bbox[1]; draw_x, draw_y = bbox[0], bbox[1] # Left-Middle anchor
                elif hasattr(font, 'getbbox'): bbox = font.getbbox(number_text); text_width = bbox[2] - bbox[0]; text_height = bbox[3] - bbox[1]; draw_x = x_pos; draw_y = y_pos - text_height // 2
                else: text_width = len(number_text) * font_size * 0.6; text_height = font_size; draw_x, draw_y = x_pos, y_pos - text_height // 2

                bg_padding = 2; rect_coords = [draw_x - bg_padding, draw_y - bg_padding, draw_x + text_width + bg_padding, draw_y + text_height + bg_padding]

                # Draw background rectangle on a temporary layer and composite
                temp_layer = Image.new('RGBA', img.size, (0,0,0,0)); temp_draw = ImageDraw.Draw(temp_layer); temp_draw.rectangle(rect_coords, fill=LABEL_BG_COLOR); img.alpha_composite(temp_layer)

                # Draw text using anchor if available
                anchor_arg = "lm" if hasattr(draw, 'textbbox') else None
                draw.text((x_pos, y_pos), number_text, fill=LABEL_COLOR, font=font, anchor=anchor_arg)
                logging.debug(f"Drew '{number_text}' at ({x_pos}, {y_pos}) based on cluster analysis (x_ratio={x_ratio:.3f}, y_ratio={y_ratio:.3f}).")

            except Exception as draw_err: logging.error(f"Error drawing number '{number_text}' at ({x_pos},{y_pos}): {draw_err}", exc_info=True)

        img = img.convert("RGB"); img.save(output_path, "PNG"); logging.info(f"Saved modified image with {len(label_positions)} numbers to: {output_path}"); return True
    except FileNotFoundError: logging.error(f"Input image not found during modification attempt: {image_path}"); return False
    except Exception as e: logging.error(f"Failed to add labels to image {image_path}: {e}", exc_info=True); return False


# --- Numerical Analysis Helpers ---

def get_cluster_leaves_recursive(linkage_matrix, node_id, num_leaves):
    """Recursively find the original leaf indices (0-based) for a given node_id."""
    if node_id < num_leaves:
        return [int(node_id)]
    else:
        cluster_idx = int(node_id - num_leaves)
        if cluster_idx >= len(linkage_matrix):
            logging.error(f"Node ID {node_id} (index {cluster_idx}) out of bounds for linkage matrix size {len(linkage_matrix)}.")
            return []
        left_child = int(linkage_matrix[cluster_idx, 0])
        right_child = int(linkage_matrix[cluster_idx, 1])
        return get_cluster_leaves_recursive(linkage_matrix, left_child, num_leaves) + \
               get_cluster_leaves_recursive(linkage_matrix, right_child, num_leaves)

def analyze_dendrogram_numerically(linkage_matrix, df_metrics_imputed, df_metrics_norm, reordered_indices, original_filenames, num_levels=3):
    """
    Performs numerical analysis of dendrogram splits.
    """
    analysis_results = []
    if linkage_matrix is None or df_metrics_imputed is None or df_metrics_norm is None or reordered_indices is None or original_filenames is None:
        logging.error("Missing data required for numerical dendrogram analysis.")
        return ["Error: Missing data for numerical analysis."]
    if not SCIPY_AVAILABLE:
        return ["Error: SciPy not available for numerical analysis."]

    num_leaves = len(reordered_indices)
    num_clusters = len(linkage_matrix)
    if num_clusters == 0:
        return ["No clustering performed (only one file?)."]

    logging.info(f"Starting numerical analysis of dendrogram ({num_clusters} merge points)...")

    # Analyze top N splits based on distance
    sorted_linkage_indices = np.argsort(linkage_matrix[:, 2])[::-1]
    num_splits_to_analyze = min(num_levels, num_clusters)

    analysis_results.append(f"**Top {num_splits_to_analyze} Numerical Splits Analysis:**")

    analyzed_nodes = set() # Keep track of nodes already analyzed as part of a larger split

    for i in range(num_splits_to_analyze):
        linkage_idx = sorted_linkage_indices[i]
        node_id = num_leaves + linkage_idx
        distance = linkage_matrix[linkage_idx, 2]

        if node_id in analyzed_nodes: continue # Skip if already covered

        left_child_id = int(linkage_matrix[linkage_idx, 0])
        right_child_id = int(linkage_matrix[linkage_idx, 1])

        # Get original indices for leaves under each child
        left_leaf_orig_indices = get_cluster_leaves_recursive(linkage_matrix, left_child_id, num_leaves)
        right_leaf_orig_indices = get_cluster_leaves_recursive(linkage_matrix, right_child_id, num_leaves)

        # Add children to analyzed set to avoid redundant analysis later
        analyzed_nodes.add(left_child_id)
        analyzed_nodes.add(right_child_id)

        if not left_leaf_orig_indices or not right_leaf_orig_indices:
            analysis_results.append(f"- Split {i+1} (Node {node_id}, Dist {distance:.3f}): Error finding leaves for one or both children.")
            continue

        # Get metric profiles
        try:
            valid_metrics_for_analysis = [m for m in KEY_METRICS_FOR_ANALYSIS if m in df_metrics_norm.columns and m in df_metrics_imputed.columns]
            if not valid_metrics_for_analysis:
                 analysis_results.append(f"- Split {i+1} (Node {node_id}, Dist {distance:.3f}): No key metrics available for analysis after imputation.")
                 continue

            left_profile_norm = df_metrics_norm.iloc[left_leaf_orig_indices][valid_metrics_for_analysis].mean()
            right_profile_norm = df_metrics_norm.iloc[right_leaf_orig_indices][valid_metrics_for_analysis].mean()
            left_profile_imputed = df_metrics_imputed.iloc[left_leaf_orig_indices][valid_metrics_for_analysis].mean()
            right_profile_imputed = df_metrics_imputed.iloc[right_leaf_orig_indices][valid_metrics_for_analysis].mean()
        except IndexError:
             logging.error(f"IndexError accessing metric data for split {i+1}. Indices might be out of bounds.")
             analysis_results.append(f"- Split {i+1} (Node {node_id}, Dist {distance:.3f}): Error accessing metric data for leaves.")
             continue
        except KeyError as ke:
             logging.error(f"KeyError accessing metric data for split {i+1}: {ke}. Check KEY_METRICS_FOR_ANALYSIS.")
             analysis_results.append(f"- Split {i+1} (Node {node_id}, Dist {distance:.3f}): Error accessing metric data (KeyError).")
             continue

        # Find metrics with biggest difference (using normalized data)
        diff_norm = (left_profile_norm - right_profile_norm).abs().sort_values(ascending=False)

        desc = f"- **Split {i+1}** (Node {node_id}, Distance {distance:.3f}): Separates {len(left_leaf_orig_indices)} files (left branch) from {len(right_leaf_orig_indices)} files (right branch)."
        if not diff_norm.empty:
            top_diff_metrics = diff_norm.head(3) # Get top 3 differentiating metrics
            desc += " Key differences (Left Avg vs Right Avg):"
            for metric, diff_val in top_diff_metrics.items():
                if diff_val > 0.05: # Only report somewhat meaningful differences
                    l_val = left_profile_imputed.get(metric, np.nan)
                    r_val = right_profile_imputed.get(metric, np.nan)
                    desc += f"\n  - *{metric}*: {l_val:.2f} vs {r_val:.2f} (Norm Diff: {diff_val:.2f})"
        else:
            desc += " No single metric showed a strong difference based on normalized averages."
        analysis_results.append(desc)

    logging.info(f"Finished numerical analysis of top {num_splits_to_analyze} splits.")
    return analysis_results


def generate_augmented_analysis_prompt(numerical_analysis_summary, file_list, branch_desc=""):
    """Generates prompt asking LLM to synthesize numerical findings with visual heatmap patterns."""
    prompt = f"""
You are analyzing a specific branch or cluster within a larger clustered heatmap visualization. The row dendrogram on the left has numbered labels (#1, #2, etc.) corresponding to major splits/branches identified earlier.

**Numerical Analysis Findings {branch_desc}:**
{numerical_analysis_summary}

**Files in this Branch/Cluster:**
{', '.join(file_list)}

**Task:**
Look at the provided **numbered heatmap image**. Focus on the rows corresponding to the files listed above and the relevant numbered branch(es) in the dendrogram.
Synthesize the numerical findings with the visual patterns you observe in the heatmap for these specific rows.

- **Confirm or Refine:** Do the heatmap colors for key metrics (like MTLD, VOCD, Avg Sent Len, Flesch, Distinct-2, Repetition-2) visually support the numerical findings (e.g., if numerical analysis says MTLD is higher in this group, do the corresponding heatmap tiles look brighter/yellower)? Mention any consistencies or discrepancies. Refer to the numbered dendrogram branches (#1, #2, etc.) if they help clarify which group you are discussing.
- **Add Visual Detail:** Describe any other notable visual patterns for this group in the heatmap (e.g., "consistent dark blue for Repetition-2", "mixed colors for Lexical Density").
- **Overall Interpretation:** Provide a concise interpretation of this specific group's characteristics, integrating both the numerical data summary and the visual heatmap evidence.

Focus *only* on the specified files and their corresponding rows in the heatmap image.
"""
    return prompt


# --- HTML Update Function (Modified for Incremental Updates & Plot Summaries) ---
def update_html_report(html_path, plot_metadata, plot_summaries=None, modified_heatmap_base64=None, llm_main_explanation=None, numerical_analysis_results=None, llm_augmented_analysis=None):
    """
    Updates the HTML report incrementally.
    - Adds individual plot summaries below their respective images if plot_summaries is provided.
    - Updates the clustered heatmap image if modified_heatmap_base64 is provided.
    - Adds clustered heatmap analysis sections if provided.
    """
    if not BS4_AVAILABLE or BeautifulSoup is None:
        logging.error("Cannot update HTML report: BeautifulSoup4 not available.")
        return False
    if not os.path.exists(html_path):
        logging.error(f"HTML file not found for updating: {html_path}")
        return False

    soup = None # Initialize soup to None
    parser_to_use = 'html.parser' # Default before checking lxml
    try:
        # --- Read HTML Content ---
        with open(html_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
            if not html_content.strip():
                 logging.error(f"HTML file '{html_path}' appears to be empty.")
                 return False

        # --- Parse HTML using lxml if available, else html.parser ---
        if LXML_AVAILABLE: parser_to_use = 'lxml'; logging.debug("Using 'lxml' parser.")
        else: logging.debug("Using 'html.parser'.")

        soup = BeautifulSoup(html_content, parser_to_use)
        logging.debug(f"Successfully parsed HTML file: {html_path} using {parser_to_use}")

        # --- Update Individual Plot Summaries ---
        if plot_summaries: # If plot summaries dictionary is provided
            logging.debug(f"Attempting to insert/update {len(plot_summaries)} plot summaries...")
            # Iterate through the summaries provided in this call
            for plot_key, summary_text in plot_summaries.items():
                if not summary_text: continue # Skip empty summaries

                # --- Corrected Image Search Logic ---
                plot_title = plot_metadata.get(plot_key, {}).get("title") # Get the title used for alt text
                target_img = None
                if plot_title:
                    target_img = soup.find('img', alt=plot_title) # Search by alt text
                    logging.debug(f"Searching for plot image with alt text: '{plot_title}'")
                else:
                    logging.warning(f"No title found for plot key '{plot_key}' in plot_metadata. Cannot find image by alt text.")

                if target_img:
                    summary_section_id = f"plot_summary_{plot_key}"
                    # Remove existing summary for this plot
                    existing_summary_div = target_img.find_next_sibling('div', id=summary_section_id)
                    if existing_summary_div:
                        logging.debug(f"Removing existing summary for plot: {plot_key}")
                        existing_summary_div.decompose()

                    # Create new summary div
                    summary_div = soup.new_tag('div', id=summary_section_id, style="margin-top: 10px; margin-bottom: 15px; padding: 10px; border: 1px solid #e0e0e0; background-color: #fafafa; border-radius: 3px;")
                    summary_title_tag = soup.new_tag('h5', style="margin-top: 0; margin-bottom: 5px; font-size: 1.05em; color: #444;")
                    summary_title_tag.string = f"LLM Summary: {plot_metadata.get(plot_key, {}).get('title', plot_key.replace('_', ' ').title())}"
                    summary_div.append(summary_title_tag)
                    summary_content_div = soup.new_tag('div', style="font-size: 0.95em; line-height: 1.5;")
                    summary_div.append(summary_content_div)

                    # Format and insert content
                    if summary_text.startswith("Error:"):
                        error_tag = soup.new_tag('i', style='color:red;')
                        error_tag.string = summary_text
                        summary_content_div.append(error_tag)
                    else:
                        try:
                            # Basic formatting: replace newlines with <br/>
                            parsed_content = BeautifulSoup(summary_text.replace('\n', '<br/>'), 'html.parser')
                            summary_content_div.append(parsed_content)
                        except Exception as parse_err:
                            logging.error(f"Error parsing summary content for {plot_key}: {parse_err}. Inserting raw text.")
                            summary_content_div.string = summary_text # Fallback

                    # Insert the summary div after the image tag
                    target_img.insert_after(summary_div)
                    logging.debug(f"Inserted/Updated summary for plot: {plot_key}")
                else:
                    logging.warning(f"Could not find image tag for plot '{plot_key}' to insert summary.")


        # --- Update Clustered Heatmap and its Analysis Sections ---
        heatmap_img = None
        image_container = None
        # Find heatmap image container
        heatmap_title = plot_metadata.get('clustered_heatmap', {}).get("title")
        if heatmap_title: heatmap_img = soup.find('img', alt=heatmap_title)
        else: logging.warning("Could not find 'clustered_heatmap' title in plot_metadata. Falling back to alt text regex search."); heatmap_img = None
        if not heatmap_img: heatmap_img = soup.find('img', alt=re.compile(r'Clustered Heatmap|Numbered Dendrogram', re.IGNORECASE))

        if heatmap_img:
            parent = heatmap_img.parent; found_container = False
            while parent and parent.name != 'body':
                h3_title = parent.find(lambda tag: tag.name == 'h3' and re.search(r'Clustered Heatmap', tag.get_text(strip=True), re.IGNORECASE))
                if parent.name == 'div' and h3_title: image_container = parent; found_container = True; break
                parent = parent.parent
            if not found_container: image_container = heatmap_img.parent # Fallback
        else: logging.warning("Clustered heatmap image not found for potential update/anchor."); image_container = None

        # Update the clustered heatmap image source if provided
        if heatmap_img and modified_heatmap_base64:
            heatmap_img['src'] = modified_heatmap_base64
            heatmap_img['alt'] = "Clustered Heatmap with Numbered Dendrogram Branches"
            logging.info("Updated clustered heatmap image source with numbered version.")

        # Define the anchor point for heatmap analysis sections
        heatmap_anchor = image_container if image_container else (heatmap_img.parent if heatmap_img else None)
        if not heatmap_anchor: logging.warning("Cannot find anchor point for heatmap analysis sections. Will append to body if possible."); heatmap_anchor = soup.body

        last_inserted_element = heatmap_anchor # Start inserting after this anchor

        # --- Function to create and insert a section ---
        def insert_analysis_section(title, section_id, content_list_or_str, style_base, after_element):
            existing_div = soup.find('div', id=section_id)
            if existing_div: logging.debug(f"Removing existing section '{section_id}'."); existing_div.decompose()
            if not content_list_or_str: logging.debug(f"No content provided for section '{title}'. Skipping insertion."); return after_element

            section_div = soup.new_tag('div', id=section_id, style=style_base)
            section_title_tag = soup.new_tag('h4', style="margin-top: 0; margin-bottom: 10px; font-size: 1.15em; color: #333;")
            section_title_tag.string = title
            section_div.append(section_title_tag)
            content_div = soup.new_tag('div', style="font-size: 0.95em; line-height: 1.55;")
            section_div.append(content_div)

            # Handle content (string or list of strings)
            if isinstance(content_list_or_str, list):
                for item in content_list_or_str:
                    p = soup.new_tag('p', style="margin-bottom: 0.6em;")
                    item_lines = str(item).split('\n')
                    for i, line in enumerate(item_lines): p.append(NavigableString(line));
                    if i < len(item_lines) - 1: p.append(soup.new_tag('br'))
                    content_div.append(p)
            elif isinstance(content_list_or_str, str):
                 if content_list_or_str.startswith("Error:"): error_tag = soup.new_tag('i', style='color:red;'); error_tag.string = content_list_or_str; content_div.append(error_tag)
                 else:
                      try: parsed_content = BeautifulSoup(content_list_or_str.replace('\n', '<br/>'), 'html.parser'); content_div.append(parsed_content)
                      except Exception as parse_err: logging.error(f"Error parsing content for section '{title}': {parse_err}. Inserting raw text."); content_div.string = content_list_or_str
            else: content_div.string = "Invalid content format."

            # Insert the section
            if after_element and after_element.parent:
                 after_element.insert_after(section_div); logging.info(f"Inserted section '{title}' (id: {section_id})."); return section_div
            else:
                 logging.warning(f"Could not insert section '{title}' - anchor invalid or detached. Appending to body.")
                 if soup.body: soup.body.append(section_div); return section_div
                 else: logging.error("Cannot append section to body as body tag was not found."); return after_element

        # --- Insert Clustered Heatmap Analysis Sections ---
        if llm_main_explanation:
            last_inserted_element = insert_analysis_section(
                title="Inferred Main Dendrogram Branch Meanings (LLM Visual Estimate)",
                section_id="dendrogram_explanation_section",
                content_list_or_str=llm_main_explanation,
                style_base="margin-top: 15px; margin-bottom: 15px; padding: 12px; border: 1px solid #ccc; background-color: #f8f8f8; border-radius: 4px;",
                after_element=last_inserted_element
            )
        if numerical_analysis_results:
            last_inserted_element = insert_analysis_section(
                title="Numerical Dendrogram Split Analysis",
                section_id="numerical_analysis_section",
                content_list_or_str=numerical_analysis_results,
                style_base="margin-top: 15px; margin-bottom: 15px; padding: 12px; border: 1px solid #ddd; background-color: #f0f0f0; border-radius: 4px;",
                after_element=last_inserted_element
            )
        if llm_augmented_analysis:
            last_inserted_element = insert_analysis_section(
                title="Detailed Analysis (Numerical + Visual Augmentation)",
                section_id="augmented_analysis_section",
                content_list_or_str=llm_augmented_analysis,
                style_base="margin-top: 15px; margin-bottom: 25px; padding: 12px; border: 1px solid #cce; background-color: #f5f5ff; border-radius: 4px;",
                after_element=last_inserted_element
            )

        # --- Save the modified HTML back to the ORIGINAL file ---
        output_html_path = html_path
        with open(output_html_path, "w", encoding="utf-8") as f:
            f.write(str(soup.prettify()))

        return True

    except FileNotFoundError:
        logging.error(f"HTML file '{html_path}' not found for updating.")
        return False
    except Exception as e:
        if "bs4.FeatureNotFound" in str(type(e)):
             logging.critical(f"CRITICAL: Failed to parse HTML with '{parser_to_use}'. Ensure parser is installed ('pip install lxml' recommended) and the HTML is valid. Error: {e}", exc_info=True)
        else:
             logging.error(f"Failed to update HTML file due to an unexpected error: {e}", exc_info=True)
        if soup is None:
             logging.error("HTML parsing failed, cannot proceed with update.")
        return False


# --- Main Execution ---
if __name__ == "__main__":
    # 0. Check Core Dependencies
    if not REQUESTS_AVAILABLE or not PIL_AVAILABLE or not BS4_AVAILABLE:
        logging.critical("CRITICAL: Missing one or more essential libraries (requests, Pillow, beautifulsoup4). Please install them.")
        sys.exit(1)

    # 1. Get Target Folder
    if len(sys.argv) < 2:
        print("Make sure your LLM backend and model is running first with at least 2K context.")
        print()
        prompt = f"Enter path to folder containing outputs and plots (or wait 5s for current: {os.getcwd()}): "
        user_path = get_input_with_timeout(prompt, 5)
        if user_path is None: target_folder = os.getcwd(); print(f"\nTimeout! Using current directory: {target_folder}")
        else:
            user_path_stripped = user_path.strip()
            if not user_path_stripped: target_folder = os.getcwd(); print(f"\nNo path entered. Using current directory: {target_folder}")
            else: target_folder = user_path_stripped; print(f"\nUsing provided path: {target_folder}")
    else:
        target_folder = sys.argv[1]
        print(f"Using folder path from command line argument: {target_folder}")

    if not os.path.isdir(target_folder):
        log_func = logging.critical if 'logging' in sys.modules else print
        log_func(f"CRITICAL Error: Target folder not found or is not a directory: {target_folder}")
        sys.exit(1)

    # Construct full paths
    html_path = os.path.join(target_folder, HTML_FILENAME)
    ini_path = os.path.join(target_folder, INI_FILENAME)
    csv_path = os.path.join(target_folder, CSV_FILENAME)
    heatmap_path = os.path.join(target_folder, HEATMAP_FILENAME)
    modified_heatmap_path = os.path.join(target_folder, MODIFIED_HEATMAP_FILENAME)
    linkage_path = os.path.join(target_folder, LINKAGE_MATRIX_FILE)
    indices_path = os.path.join(target_folder, REORDERED_INDICES_FILE)
    filenames_path = os.path.join(target_folder, ORIGINAL_FILENAMES_FILE) # Optional
    plots_dir = os.path.join(target_folder, "plots") # Define plots dir path
    plot_metadata_path = os.path.join(target_folder, PLOT_METADATA_FILENAME) # Path for plot metadata

    # --- Attempt to open HTML report ---
    if os.path.isfile(html_path):
        try:
            print(f"\nAttempting to open HTML report: {html_path}")
            webbrowser.open(f"file://{os.path.realpath(html_path)}")
            print("Report should open in your default browser shortly...")
        except Exception as e:
            logging.warning(f"Could not automatically open HTML report: {e}")
            print(f"Warning: Could not automatically open HTML report. Please open manually: {html_path}")
    else:
        logging.warning(f"HTML report not found at {html_path}. Cannot open automatically.")
        print(f"Warning: HTML report not found at {html_path}. Cannot open automatically.")

    # Check required files exist
    required_files = [html_path, ini_path, csv_path, heatmap_path, linkage_path, indices_path, plot_metadata_path] # Added plot_metadata_path
    missing_files = [f for f in required_files if not os.path.isfile(f)]
    if missing_files:
        for f in missing_files: logging.critical(f"CRITICAL: Required file not found: {f}")
        logging.critical("Ensure lexicon_analyzer.py and lexicon_visualizer.py (modified to save .npy and .json files) have been run successfully.")
        sys.exit(1)

    # 2. Load INI Configuration for LLM
    config = load_config(ini_path)
    if config is None: sys.exit(1)
    llm_config, llm_enabled = get_llm_config(config)
    if not llm_enabled or llm_config is None:
        logging.critical("Local LLM is not enabled or configured correctly in the INI file. This script requires it.")
        sys.exit(1)
    if not llm_config.get('api_base'):
        logging.critical("CRITICAL: 'api_base' missing in [LocalLLM] section of INI file.")
        sys.exit(1)

    # 3. Load Numerical Data & Plot Metadata
    logging.info("--- Loading Numerical Data & Plot Metadata ---")
    df_metrics_raw = load_metric_data(csv_path) # Now respects separator and filters
    linkage_matrix = load_npy_file(linkage_path)
    reordered_indices = load_npy_file(indices_path)
    original_filenames_map = load_json_file(filenames_path)
    plot_metadata = load_json_file(plot_metadata_path) # Load plot metadata

    if df_metrics_raw is None or linkage_matrix is None or reordered_indices is None:
        logging.critical("Failed to load essential numerical data. Cannot proceed.")
        sys.exit(1)
    if not plot_metadata:
        logging.warning(f"Plot metadata file '{PLOT_METADATA_FILENAME}' not found or failed to load. Cannot generate individual plot summaries.")
        plot_metadata = {} # Use empty dict if missing

    # <<< FIX: Derive original_filenames list AFTER filtering in load_metric_data >>>
    if 'Filename' in df_metrics_raw.columns:
        original_filenames = df_metrics_raw['Filename'].tolist()
        logging.info(f"Using {len(original_filenames)} filenames from filtered CSV data.")
    else: # Should not happen if load_metric_data worked, but fallback
        original_filenames = [f"Index {i}" for i in df_metrics_raw.index]
        logging.info(f"Using {len(original_filenames)} indices as filenames.")

    # Rebuild map based on the *actual* data loaded and filtered
    original_filenames_map = {i: fname for i, fname in enumerate(original_filenames)}
    # --- END FIX ---


    # 4. Preprocess Metric Data
    logging.info("--- Preprocessing Metric Data ---")
    df_metrics_imputed, df_metrics_norm = preprocess_metrics_for_analysis(df_metrics_raw)
    if df_metrics_imputed is None or df_metrics_norm is None:
        logging.critical("Metric preprocessing failed.")
        sys.exit(1)

    # --- Initialize storage for results ---
    plot_summaries = {}
    llm_main_visual_explanation = None
    numerical_analysis_results = None
    llm_augmented_analysis = None
    modified_heatmap_base64 = None # Will be generated later
    llm_error_occurred = False # Flag to stop on critical LLM error
    base_heatmap_base64 = None # To store original heatmap encoding

    # --- 5. Generate Summaries for Other Plots (Iterative Update) ---
    logging.info("--- Generating LLM Summaries for Individual Plots ---")
    base_max_tokens = int(llm_config.get('max_tokens', DEFAULT_MAX_TOKENS))
    plot_summary_tokens = int(base_max_tokens * PLOT_SUMMARY_MAX_TOKENS_MULTIPLIER)
    logging.info(f"Requesting max_tokens={plot_summary_tokens} (x{PLOT_SUMMARY_MAX_TOKENS_MULTIPLIER} default) for individual plot summaries.")

    # --- UPDATED plot_order to match visualizer metadata keys ---
    plot_order = [
        'normalized_diversity_bar',
        'mtld_distribution',
        'mtld_vs_vocd_scatter',         # Corrected key
        'unique_vs_sentlen_scatter',    # Corrected key
        'grouped_profile_radar',
        'metrics_heatmap',
        'correlation_matrix',
        'parallel_coordinates',
        'readability_profiles',         # Corrected key
        'grouped_emotion_bars',         # Corrected key
        'nrclex_scores_summary',        # Corrected key
        # 'clustered_heatmap' # Process this last
    ]

    for plot_key in plot_order:
        if llm_error_occurred: break # Stop if a previous LLM call failed critically
        if plot_key not in plot_metadata:
             # This warning should ideally not appear now if visualizer ran correctly
             logging.warning(f"Skipping summary for plot key '{plot_key}': Not found in plot metadata.")
             continue

        metadata = plot_metadata[plot_key]
        plot_filename_base = metadata.get("filename_base")
        plot_title = metadata.get("title")
        plot_desc = metadata.get("description")

        # <<< FIX: Check if filename_base is None (indicating plot failed/skipped) >>>
        if not plot_filename_base:
            logging.warning(f"Skipping summary for plot key '{plot_key}': Plot was not generated (filename_base is null in metadata).")
            plot_summaries[plot_key] = "Info: Plot generation was skipped or failed." # Store info
            update_success = update_html_report(html_path, plot_metadata, plot_summaries={plot_key: plot_summaries[plot_key]})
            if not update_success: llm_error_occurred = True; break
            continue
        # <<< END FIX >>>

        if not plot_title or not plot_desc:
            logging.warning(f"Skipping summary for plot key '{plot_key}': Missing title or description in metadata.")
            continue

        plot_path = os.path.join(plots_dir, f"{plot_filename_base}.png")
        if not os.path.exists(plot_path):
            logging.warning(f"Skipping summary for plot key '{plot_key}': Image file not found at {plot_path}")
            plot_summaries[plot_key] = "Error: Plot image file not found." # Store error
            update_success = update_html_report(html_path, plot_metadata, plot_summaries={plot_key: plot_summaries[plot_key]})
            if not update_success: llm_error_occurred = True; break
            continue

        logging.info(f"Generating summary for plot: {plot_key} ({plot_filename_base}.png)")
        plot_base64 = encode_image_to_base64(plot_path)
        if not plot_base64:
            logging.error(f"Failed to encode image for plot: {plot_key}")
            plot_summaries[plot_key] = "Error: Failed to encode plot image."
            update_success = update_html_report(html_path, plot_metadata, plot_summaries={plot_key: plot_summaries[plot_key]})
            if not update_success: llm_error_occurred = True; break
            continue

        prompt = generate_plot_summary_prompt(plot_title, plot_desc)
        summary = call_local_llm_api_multimodal(
            prompt,
            plot_base64,
            llm_config,
            max_tokens_override=plot_summary_tokens
        )
        plot_summaries[plot_key] = summary # Store summary (or error string)

        # --- Incremental HTML Update ---
        update_success = update_html_report(html_path, plot_metadata, plot_summaries={plot_key: summary})
        if update_success: logging.info(f"HTML updated incrementally after summary for plot: {plot_key}")
        else: logging.error(f"Failed incremental HTML update after plot summary: {plot_key}"); llm_error_occurred = True; break

        if summary and summary.startswith("Error:"):
            logging.error(f"LLM failed to generate summary for plot: {plot_key}. Stopping LLM calls.")
            llm_error_occurred = True; break

    # --- 6. Clustered Heatmap Analysis (Visual) ---
    if not llm_error_occurred:
        logging.info("--- Generating Prompt and Calling Local LLM for Main Dendrogram Visual Explanation ---")
        base_heatmap_base64 = encode_image_to_base64(heatmap_path)
        if base_heatmap_base64:
            num_labels_to_generate = min(MAX_LABELS_TO_PLACE, len(linkage_matrix))
            main_prompt = generate_dendrogram_interpretation_prompt(num_labels_to_generate)
            main_explanation_tokens = base_max_tokens * 2
            logging.info(f"Requesting max_tokens={main_explanation_tokens} (x2 default) for main dendrogram explanation ({num_labels_to_generate} points).")

            llm_main_visual_explanation = call_local_llm_api_multimodal(
                main_prompt,
                base_heatmap_base64,
                llm_config,
                max_tokens_override=main_explanation_tokens
            )
            update_success = update_html_report(html_path, plot_metadata, plot_summaries, llm_main_explanation=llm_main_visual_explanation)
            if update_success: logging.info("HTML updated incrementally after main visual explanation.")
            else: logging.error("Failed incremental HTML update after main visual explanation."); llm_error_occurred = True
            if llm_main_visual_explanation and llm_main_visual_explanation.startswith("Error:"): llm_error_occurred = True
        else:
            logging.error("Failed to encode base heatmap image for visual explanation.")
            llm_main_visual_explanation = "Error: Failed to load base heatmap image."
            llm_error_occurred = True
            update_html_report(html_path, plot_metadata, plot_summaries, llm_main_explanation=llm_main_visual_explanation)


    # --- 7. Modify Image (Numbering) ---
    num_labels_from_llm = 0
    if llm_main_visual_explanation and not llm_main_visual_explanation.startswith("Error:"):
        num_labels_from_llm = count_numbered_items(llm_main_visual_explanation)
        if num_labels_from_llm == 0:
            logging.warning("LLM call for visual explanation succeeded but returned no numbered items.")
            logging.warning(f"LLM Response (first 200 chars): {llm_main_visual_explanation[:200]}...")
            print("\nWARNING: LLM did not provide a numbered analysis for the heatmap dendrogram.")
            print("         Check if the configured 'transformer_model' supports image input and if the LLM backend is running correctly.")
            print("         Skipping numbering on the heatmap image.\n")
    else:
        logging.warning("LLM main visual explanation failed or is empty. Cannot determine number of labels to place.")

    logging.info(f"--- Modifying Heatmap Image (Attempting {num_labels_from_llm} Labels) ---")
    success_labeling = add_numbers_to_image(
        heatmap_path,
        modified_heatmap_path,
        num_labels_from_llm,
        linkage_matrix,
        reordered_indices
    )

    # --- 8. Encode Modified Image ---
    if success_labeling and os.path.exists(modified_heatmap_path):
        modified_heatmap_base64 = encode_image_to_base64(modified_heatmap_path)
        if not modified_heatmap_base64:
            logging.error("Successfully labeled image, but failed to encode the modified version.")
            modified_heatmap_base64 = base_heatmap_base64 # Fallback
    elif not success_labeling:
        logging.error("Failed to add labels to the image.")
        modified_heatmap_base64 = base_heatmap_base64 # Fallback
    else: # No labels needed, use original
         modified_heatmap_base64 = base_heatmap_base64

    # --- 9. Perform Numerical Analysis ---
    if not llm_error_occurred: # Only proceed if previous steps okay
        logging.info("--- Performing Numerical Analysis of Dendrogram Splits ---")
        numerical_analysis_results = analyze_dendrogram_numerically(
            linkage_matrix, df_metrics_imputed, df_metrics_norm,
            reordered_indices, original_filenames,
            num_levels=NUM_SUB_BRANCH_LEVELS_TO_ANALYZE
        )
        update_success = update_html_report(html_path, plot_metadata, plot_summaries, modified_heatmap_base64, llm_main_visual_explanation, numerical_analysis_results)
        if update_success: logging.info("HTML updated incrementally after numerical analysis.")
        else: logging.error("Failed incremental HTML update after numerical analysis."); llm_error_occurred = True


    # --- 10. Generate Augmented Analysis ---
    if not llm_error_occurred and numerical_analysis_results and not numerical_analysis_results[0].startswith("Error:"):
        logging.info("--- Generating Prompt and Calling Local LLM for Augmented Analysis ---")
        numerical_summary_for_prompt = "\n".join(numerical_analysis_results)
        augmented_prompt = generate_augmented_analysis_prompt(numerical_summary_for_prompt, original_filenames, branch_desc=" (Top Splits)")
        augmented_tokens = int(base_max_tokens * DETAILED_ANALYSIS_MAX_TOKENS_MULTIPLIER)
        logging.info(f"Requesting max_tokens={augmented_tokens} for augmented analysis.")

        llm_augmented_analysis = call_local_llm_api_multimodal(
            augmented_prompt,
            modified_heatmap_base64, # Use the NUMBERED image encoding
            llm_config,
            max_tokens_override=augmented_tokens
        )
        if llm_augmented_analysis and llm_augmented_analysis.startswith("Error:"):
             logging.error(f"LLM augmented analysis failed: {llm_augmented_analysis}")
             llm_error_occurred = True
        elif not llm_augmented_analysis:
             logging.warning("LLM augmented analysis returned empty.")
        else:
             logging.info("Successfully generated augmented analysis.")

        # --- Final HTML Update ---
        logging.info("--- Performing Final HTML Update ---")
        success_html = update_html_report(
            html_path, plot_metadata, plot_summaries, modified_heatmap_base64,
            llm_main_visual_explanation, numerical_analysis_results,
            llm_augmented_analysis
        )
        if not success_html:
             logging.error("Failed during FINAL HTML update.")
             sys.exit(1)

    elif llm_error_occurred:
         logging.error("Skipping augmented analysis and final HTML update due to previous errors.")
         sys.exit(1)
    else:
         logging.warning("Skipping LLM augmented analysis because numerical analysis failed or was empty.")
         logging.info("--- Performing Final HTML Update (without augmented analysis) ---")
         success_html = update_html_report(
             html_path, plot_metadata, plot_summaries, modified_heatmap_base64,
             llm_main_visual_explanation, numerical_analysis_results,
             None # No augmented analysis
         )
         if not success_html:
              logging.error("Failed during FINAL HTML update (without augmented analysis).")
              sys.exit(1)


    # --- Final Status Logging ---
    if llm_error_occurred:
         logging.error("Dendrogram transformation script finished with critical errors during LLM calls or HTML updates.")
         sys.exit(1)
    elif not success_labeling:
         logging.warning("Dendrogram transformation script finished, but failed to label the image. HTML updated with analysis.")
    else:
         logging.info("Dendrogram transformation script finished successfully.")


    logging.info("--- Script Finished ---")