# Configuration for Offline Interpretation

[General]
# Choose the interpretation mode:
# rules      = Use built-in simple rule-based interpretation.
# local_llm  = Use a local LLM backend (like KoboldCpp).
# api = Use external LLM API. Disabled for now.
# mode = rules

[LocalLLM]
# Settings for using a local LLM backend (like KoboldCpp)
enabled = true

# The full URL to your running KoboldCpp API endpoint (usually /api/v1)
# Ensure KoboldCpp is running BEFORE starting this script if enabled=true.
api_base = http://localhost:5001/v1/chat/completions

# Optional: Model name (often ignored by KoboldCpp if model is preloaded, but good for logging)
## Best current model: https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF
model = gemma-3-27b-it-abliterated.q8_0.gguf

# Optional: Generation parameters
temperature = 1.0
max_tokens = 256
# top_p = 0.9
# top_k = 40
# --- Add other local LLM providers similarly if they offer compatible endpoints ---
