# Text Analysis Suite

## Creator
**https://github.com/DoorBotEvilTwin/**

## Description

This Python project provides an offline suite of scripts for analyzing text files (`.txt`) to calculate linguistic metrics, visualize the results, and optionally generate interpretations.

1.  **`lexicon_analyzer.py`**: Analyzes a folder of text files (`.txt`) to calculate a wide range of linguistic and readability metrics for each file. It also computes collective metrics assessing diversity/similarity across the entire set. Outputs results in `.csv`, `.txt`, and a basic `.html` report with metadata and a sortable table.
2.  **`lexicon_visualizer.py`**: Takes the `.csv` output from the analyzer, generates various plots (bar charts, scatter plots, heatmaps, etc.), and embeds them directly into the `.html` report along with detailed metric and plot descriptions, creating a rich visual summary.
3.  **`lexicon_processor.py` (Optional)**: Further processes the data and the HTML report generated by the visualizer to add interpretations. It operates entirely offline and offers two modes configured via `lexicon_processor.ini`:
    *   **Rule-Based Mode (Default & Always Run):** Uses predefined Python logic to generate simple descriptive labels for file clusters and brief comparative summaries for individual files based on metric thresholds and dataset statistics. These results are always added to the HTML report.
    *   **Local LLM Mode (Optional):** If enabled in the `.ini` file, this mode connects to a locally running Large Language Model backend (like KoboldCpp) via its API endpoint. It sends prompts (with anonymized file IDs) containing processed metrics to the local LLM to generate potentially more nuanced cluster labels and file summaries. Requires the backend server to be running separately. These results are added *after* the rule-based results in the HTML report.
    *   The script **updates the existing** `_LEXICON.html` file by appending interpretation sections after the clustered heatmap.

The suite is particularly useful for analyzing and comparing outputs from Large Language Models (LLMs), but can be applied to any collection of text documents seeking detailed linguistic profiling and interpretation.

## Features: Calculated Metrics

The main script (`lexicon_analyzer.py`) calculates the following metrics for each input text file. Understanding these metrics provides insight into the text's complexity, style, and vocabulary usage.

**A. Identification & Basic Counts**

1.  **Filename:** The unique identifier for the input text file.

2.  **Total Words:** The total count of tokens (words) identified in the file using NLTK's standard word tokenizer. This forms the basis (N) for many ratio calculations.

3.  **Total Characters:** The raw count of all characters in the file, including spaces, punctuation, and line breaks. Useful for a basic size assessment.

4.  **Unique Words:** The count of distinct word *types* (case-insensitive) found in the file. This represents the size of the vocabulary (V) used.

**B. Readability & Sentence Structure**

5.  **Average Sentence Length:** The average number of words per sentence (Total Words / Number of Sentences). Provides a basic measure of syntactic complexity.

6.  **Flesch Reading Ease:** A widely used readability formula (0-100 scale) based on average sentence length and average syllables per word. Scores around 60-70 are considered plain English. Higher scores indicate easier text; very low scores indicate complex, potentially difficult-to-read text.

**C. Lexical Diversity (Word Level)**
    *_(Lexical diversity metrics assess the variety and richness of vocabulary used.)_*
	
7.  **TTR (Type-Token Ratio):** The simplest diversity measure: `Unique Words / Total Words`. A value closer to 1 indicates that a larger proportion of the words used are unique *within that specific text*. **Limitation:** Highly sensitive to text length (longer texts naturally have lower TTR). Best used only for comparing texts of very similar lengths.

8.  **RTTR (Root TTR):** An early attempt to correct TTR for length: `Unique Words / sqrt(Total Words)`. Reduces length sensitivity compared to TTR but is generally considered less robust than measures like VOCD or MTLD.

9.  **Herdan's C (LogTTR):** Another length normalization attempt, also known as LogTTR: `log(Unique Words) / log(Total Words)`. It assumes a specific logarithmic relationship between vocabulary growth and text length. Higher values suggest greater diversity relative to length *under this specific model*.

10. **MATTR (Moving-Average TTR, W=100):** Calculates TTR over sliding windows of a fixed size (default 100 words) and averages the results. This measures *local* lexical diversity, indicating how varied the vocabulary is within smaller text segments. It's less sensitive to overall text length than raw TTR.

11. **MSTTR (Mean Segmental TTR, W=100):** Calculates TTR over sequential, *non-overlapping* segments (default 100 words) and averages them. Provides a different perspective on segmental diversity compared to the overlapping windows of MATTR.

12. **VOCD (HD-D):** A sophisticated and robust measure designed to be largely independent of text length. It models the theoretical relationship between TTR and text length using hypergeometric distribution probabilities (specifically, the HD-D variant). Higher scores indicate greater underlying vocabulary richness, allowing for fairer comparison between texts of different lengths.

13. **MTLD (Measure of Textual Lexical Diversity):** Another highly robust, length-independent measure. It calculates the average number of sequential words required for the local TTR (calculated over the growing sequence) to fall below a specific threshold (typically 0.72). A higher MTLD score indicates greater diversity, meaning longer stretches of text were needed before vocabulary repetition became significant. Often preferred alongside VOCD for rigorous comparisons.

14. **Yule's K:** A measure focusing on vocabulary richness via word *repetition* patterns (derived from the frequency distribution). Unlike TTR-based measures, it's less sensitive to words occurring only once. Lower values indicate higher repetition of the most frequent words (lower diversity); higher values indicate more even word usage across the vocabulary (higher diversity). Calculated manually based on the formula `10^4 * [ Σ(f_i * (f_i - 1)) ] / [ N * (N - 1) ]`.

15. **Simpson's D:** Originally an ecological diversity index, applied here it measures the probability that two words selected randomly from the text will be of the same *type*. It reflects vocabulary *concentration* or the dominance of frequent words. Higher values (closer to 1) indicate *higher* concentration and therefore *lower* lexical diversity. Calculated manually based on the formula `Σ [ n(n-1) ] / [ N(N-1) ]`.

16. **Lexical Density:** The ratio of content words (nouns, verbs, adjectives, adverbs) to the total number of words. A higher density often suggests text that is more informationally packed or descriptive, as opposed to text with a higher proportion of function words (pronouns, prepositions, conjunctions). Requires Part-of-Speech tagging.

**D. N-gram Level Diversity & Repetition (Phrase Level)**
    *_(These metrics assess the variety and repetition of short word sequences (phrases).)_*
	
17. **Distinct-2:** The proportion of unique bigrams (adjacent 2-word sequences) relative to the total number of bigrams in the text. A higher value indicates greater variety in word pairings and less reliance on repeated phrases.

18. **Repetition-2:** The proportion of bigram *tokens* that are repetitions of bigram *types* already seen earlier in the text. A higher value indicates more immediate phrase repetition, potentially impacting fluency.

19. **Distinct-3:** The proportion of unique trigrams (adjacent 3-word sequences) relative to the total number of trigrams. A higher value indicates greater variety in short three-word phrases.

20. **Repetition-3:** The proportion of trigram *tokens* that are repetitions of trigram *types* already seen earlier. Higher values indicate more repetition of three-word phrases.

**E. Collective Metrics (Calculated Across the Entire Set - Reported Separately)**
    *_(These metrics assess the overall diversity or similarity *between* the documents in the analyzed set.)_*
	
21. **Self-BLEU:** The average pairwise BLEU score calculated between all pairs of documents in the set. BLEU measures N-gram overlap. A higher Self-BLEU score indicates that the documents in the set are textually very similar to each other (using similar phrasing), suggesting low diversity *within the set*.

22. **Avg Pairwise Cosine Similarity:** Calculates a vector embedding (representing semantic meaning) for each document and finds the average cosine similarity between all pairs of embeddings. Measures *semantic* similarity across documents. Higher scores (closer to 1) indicate the documents discuss very similar topics or convey similar meanings, suggesting low semantic diversity *within the set*.

23. **Avg Distance to Centroid:** Calculates the average distance (using cosine distance) of each document's embedding from the mean embedding (centroid) of the entire set. Measures the semantic *spread* or dispersion of the documents. Higher values indicate the documents are more spread out semantically and cover a wider range of meanings (high semantic diversity *within the set*).

*(Note: Metrics in section E are calculated only if 2 or more files are successfully processed.)*

## Requirements

*   **Python:** Version 3.8 or higher recommended.
*   **Libraries:** The scripts rely on several external Python libraries. See installation steps.
*   **Internet Connection:** Required only for downloading libraries and potentially NLTK/spaCy data on first use. The core analysis, visualization, and interpretation scripts run offline.
*   **Local LLM Backend (Optional, for `lexicon_processor.py`):** To use the Local LLM interpretation mode, you need a separate LLM server application (like KoboldCpp, LM Studio, Oobabooga with API enabled) running on your machine or local network.

## Installation

1.  **Ensure Python and Pip are installed:** Make sure you have a working Python installation and that the `pip` package installer is available. If `pip` isn't recognized directly in your terminal, you can often use `python -m pip` or `py -m pip`.

2.  **Install Core Analysis Dependencies (`lexicon_analyzer.py`):**
    ```bash
    # Use 'py -m pip' if 'pip' is not recognized directly
    py -m pip install lexical-diversity nltk sentence-transformers torch numpy scipy pandas textstat spacy
    ```
    *(Note: This might take some time and download a significant amount of data, primarily for `torch`. Estimated size: **1 GB - 2.5 GB+**)*

3.  **Download spaCy Model:**
    ```bash
    py -m spacy download en_core_web_sm
    ```

4.  **Install Visualization Dependencies (`lexicon_visualizer.py`):**
    ```bash
    py -m pip install matplotlib seaborn pandas scikit-learn scipy
    ```
    *(Note: `pandas`, `scipy`, `scikit-learn` are listed again for clarity. Estimated size: **150 MB - 350 MB**)*

5.  **Install Interpretation Dependencies (`lexicon_processor.py`, Optional but Recommended):**
    ```bash
    py -m pip install beautifulsoup4 configparser requests
    ```
    *(Note: `configparser` might be built-in. `requests` is needed only if using Local LLM mode. Estimated size: **~1 MB**)*

6.  **NLTK Data (Automatic Download):** `lexicon_analyzer.py` will attempt to automatically download required NLTK data packages (`punkt`, `averaged_perceptron_tagger`) on its first run if they are not found.

## Usage

### 1. Running the Analysis (`lexicon_analyzer.py`)

1.  **Save the Script:** Save `lexicon_analyzer.py`.
2.  **Prepare Text Files:** Create a folder with your `.txt` files.
3.  **Run from Terminal:** `cd` to the script directory.
4.  **Execute:** `py lexicon_analyzer.py`
5.  **Enter Folder Path:** Provide the path to your text file folder.
6.  **Wait:** Monitor console output.

### 2. Running the Visualization (`lexicon_visualizer.py`)

1.  **Save the Script:** Save `lexicon_visualizer.py` in the same directory.
2.  **Ensure Analysis is Complete:** `_LEXICON.csv` and `_LEXICON.html` must exist in the text file folder.
3.  **Run from Terminal:** `cd` to the script directory.
4.  **Execute:** `py lexicon_visualizer.py`
5.  **Enter Folder Path:** Provide the path to the folder containing `_LEXICON.csv` and `.html`.
6.  **View Report:** Open the **updated** `_LEXICON.html` in a web browser.

### 3. Running the Interpretation (`lexicon_processor.py`, Optional)

1.  **Save the Script:** Save `lexicon_processor.py` in the same directory.
2.  **Configure Interpretation:** Create/Edit the `lexicon_processor.ini` file (see Configuration Notes below). Decide if you want to enable the Local LLM mode (`enabled = true` in `[LocalLLM]`) and set the correct `api_base` URL for your running backend.
3.  **Start Local LLM Backend (If using):** If you set `enabled = true` for `[LocalLLM]`, ensure your backend server (e.g., KoboldCpp) is running and accessible at the `api_base` URL specified in the `.ini` file.
4.  **Ensure Visualization is Complete:** Make sure `_LEXICON.html` (containing the clustered heatmap) exists.
5.  **Run from Terminal:** `cd` to the script directory.
6.  **Execute:** `py lexicon_processor.py`
7.  **Enter Folder Path:** Provide the path to the folder containing `_LEXICON.csv` and `.html`.
8.  **View Interpreted Report:** The script generates rule-based interpretations and optionally calls the local LLM. It then **updates** the existing `_LEXICON.html` file by appending the interpretation section(s) after the clustered heatmap. Refresh or reopen `_LEXICON.html` in your browser.

## Output Files

### Output Files (from `lexicon_analyzer.py`)

Generated inside the **folder containing your input `.txt` files**:

1.  **`_LEXICON.csv`:** Comma-separated values file with individual metrics per row. Collective metrics appended. Used by subsequent scripts.
2.  **`_LEXICON.txt`:** Formatted text file with the same data for console viewing.
3.  **`_LEXICON.html`:** An initial HTML file containing analysis metadata (timestamp, file count, duration), the results table (with sortable headers enabled via embedded JavaScript), collective metrics, and basic styling. This file is **updated** by `lexicon_visualizer.py` and potentially again by `lexicon_processor.py`.

Generated in the **directory where the script is run**:

4.  **`_LEXICON_errors.log`:** Log file with execution details, warnings, and errors from all scripts (visualizer and processor append to it). Check this if metrics show as "FAIL" or plots are missing.

### Output Files & Visualizations (from `lexicon_visualizer.py`)

Updates `_LEXICON.html` by embedding plots and descriptions. Also saves individual plot images inside a `plots` subdirectory within the **folder containing your input `.txt` files**:

1.  **`normalized_diversity_metrics_bar_chart.png`:**
    *   **Description:** Compares key diversity metrics (TTR, MTLD, VOCD, Distinct-2) across all files.
    *   **How To Read:** Values are normalized (0-1 scale) to allow visual comparison despite different original scales. Higher bars generally indicate higher diversity for that specific metric relative to the other files in the set. Helps quickly identify files that are consistently high or low on these common measures.

2.  **`mtld_distribution.png`:**
    *   **Description:** Shows the distribution (histogram and density curve) of MTLD scores across all analyzed files.
    *   **How To Read:** The histogram bars show how many files fall into specific MTLD score ranges. The curve estimates the underlying probability distribution. This helps understand the overall range, central tendency (peak), and spread of MTLD scores in your dataset. Is the diversity generally high or low? Is it consistent or highly variable?

3.  **`mtld_vs_avg_sent_len_scatter.png`:**
    *   **Description:** Plots each file as a point based on its MTLD score (Y-axis) and its Average Sentence Length (X-axis).
    *   **How To Read:** Look for patterns or trends. Is there a positive correlation (longer sentences associated with higher MTLD), negative correlation, or no clear relationship? Clusters of points might indicate groups of files with similar lexical diversity *and* sentence complexity characteristics.

4.  **`vocd_vs_unique_words_scatter.png`:**
    *   **Description:** Plots each file based on its VOCD score (Y-axis) and its total number of Unique Words (X-axis).
    *   **How To Read:** Since VOCD is designed to be length-independent, ideally there shouldn't be a strong correlation with raw unique word count (which *is* length-dependent). This plot helps visually verify that relationship and see if files with similar unique word counts still exhibit a range of VOCD scores.

5.  **`grouped_profile_radar_chart.png`:**
    *   **Description:** Creates average "fingerprints" for groups of files. Files are grouped into quartiles (Low, Mid-Low, Mid-High, High) based on their MTLD scores. Each axis represents a different key metric (normalized and sometimes inverted so outward means "more diverse/complex/readable"). Shows the *average* profile for files within each MTLD diversity tier.
    *   **How To Read:** Compare the shapes of the polygons for the different MTLD groups. Does the "High MTLD" group consistently score higher on other diversity/complexity metrics? Are there trade-offs (e.g., does higher diversity correlate with lower readability in this dataset)? This shows average tendencies for different diversity levels.

6.  **`metrics_heatmap.png`:**
    *   **Description:** Provides a grid view where rows are files and columns are metrics. The color intensity of each cell represents the normalized value (0-1) of that metric for that file (typically, brighter colors like yellow mean higher normalized values, darker colors like purple mean lower).
    *   **How To Read:** Look for rows (files) or columns (metrics) with consistently bright or dark colors. Identify blocks of similar colors, which might indicate groups of files with similar metric profiles. Useful for spotting overall patterns and relationships visually at a glance.

7.  **`metrics_correlation_heatmap.png`:**
    *   **Description:** Shows the pairwise Pearson correlation coefficient (-1 to +1) between all numeric metrics calculated across all files.
    *   **How To Read:** Colors indicate the strength and direction of the correlation (e.g., warm colors like red for strong positive correlation, cool colors like blue for strong negative). The number in each cell is the correlation coefficient. Look for strongly correlated metrics (measuring similar underlying properties) or strongly anti-correlated metrics (measuring opposite properties). Helps understand redundancy and relationships between measures in *your specific dataset*.

8.  **`parallel_coordinates_plot.png`:**
    *   **Description:** A "super-chart" visualizing multiple key metrics simultaneously. Each file is represented by a line that connects points on parallel vertical axes. Each axis represents a different metric, normalized to a 0-1 scale (with Repetition/Simpson's D inverted for consistent interpretation where higher=better/more diverse). Lines are color-coded based on the file's MTLD quartile group.
    *   **How To Read:** Files with similar overall profiles across the selected metrics will have lines that follow similar paths and cluster together visually. The color-coding helps see if files within the same MTLD group exhibit similar patterns across other metrics (e.g., do 'High MTLD' lines generally stay high on other diversity axes?). Outliers will have lines that deviate significantly. Observe group trends rather than trying to trace every individual line.

9.  **`clustered_metrics_heatmap.png`:**
    *   **Description:** An enhanced heatmap where both the rows (files) and columns (metrics) have been reordered using hierarchical clustering based on their similarity. Similar files are placed near each other, and metrics that behave similarly across the files are placed near each other. Dendrograms (tree diagrams) alongside show the clustering hierarchy.
    *   **How To Read:** This chart is excellent for identifying distinct groups (clusters) of files that share similar linguistic profiles across *all* measured dimensions. Look for blocks of color indicating these groups. Observe which metrics cluster together – this reinforces findings from the correlation matrix about related measures. **Interpret the characteristics of a file cluster by examining the typical color patterns (high/low normalized values) across the metric columns for that block.** The dendrograms show how clusters are related but don't contain descriptive labels themselves. *(Rule-based and/or Local LLM interpretations for clusters and files are added to the HTML report by `lexicon_processor.py` if run).*

### Output Files (from `lexicon_processor.py`, Optional)

Generated in the **directory where the scripts are run**:

10. **`lexicon_processor.ini`:** Configuration file to enable/disable Local LLM interpretation mode and set its parameters (like the `api_base` URL). **You need to create/edit this file.**

Updates `_LEXICON.html` inside the **folder containing your input `.txt` files**:

11. **Appended Interpretation Section:** If run, this script adds a new section titled "Generated Interpretations" to the *end* of the `_LEXICON.html` file (after the clustered heatmap). This section contains:
    *   **Rule-Based Interpretations:** Always generated. Includes simple descriptive labels for clusters and comparative summaries for individual files.
    *   **Local LLM Interpretations (Optional):** Generated only if `enabled = true` in the `[LocalLLM]` section of the `.ini` file and the connection is successful. Includes potentially more nuanced labels and summaries generated by the configured local LLM. This section appears *after* the Rule-Based section if generated.

## Configuration Notes

*   **Analyzer:** Default settings (N-gram sizes, window sizes, embedding model) can be changed by editing variables near the top of `lexicon_analyzer.py`.
*   **Processor (Optional):** Interpretation behavior is controlled via `lexicon_processor.ini`.
    *   Create this file in the same directory as the scripts.
    *   The `[LocalLLM]` section controls the optional local LLM mode.
    *   Set `enabled = true` to attempt connection to a local LLM backend. Set to `false` to only use the built-in rule-based interpretation.
    *   Set `api_base` to the **full URL** of your running local LLM's OpenAI-compatible API endpoint (e.g., `http://127.0.0.1:5001/v1/chat/completions` for KoboldCpp). Check your backend's documentation/startup logs for the correct URL.
    *   Optionally adjust `model`, `temperature`, `max_tokens`.

## Potential Issues

*   **Long Runtime:** Processing many large files (`lexicon_analyzer.py`) can be time-consuming. Local LLM calls (`lexicon_processor.py`) can also be slow depending on the model and hardware.
*   **Memory Usage:** Loading embeddings (`lexicon_analyzer.py`) or large local LLMs can consume significant RAM.
*   **Library Conflicts/Environment:** Ensure all dependencies are installed in the *same* Python environment.
*   **Local LLM Backend (`lexicon_processor.py`):**
    *   Requires a separate application (KoboldCpp, etc.) to be installed, configured, and running *before* executing the processor script.
    *   Ensure the `api_base` URL in the `.ini` file exactly matches the endpoint provided by your running backend. Check for 404 errors in the log if connections fail.
    *   The backend server might crash or have internal errors (e.g., `WinError 10053`, sending non-JSON responses), interrupting the processor script. Check the backend's console for errors. The processor script attempts retries but may fail if the backend issue persists.
    *   Local models vary greatly in quality; interpretations may be less accurate or coherent than large cloud models.
*   **Rule-Based Interpretation Quality (`lexicon_processor.py`):** The built-in rules are simple heuristics. The generated labels and summaries might be generic or less insightful than LLM-based ones. Thresholds may need tuning for specific datasets.
*   **HTML Structure Dependency (`lexicon_processor.py`):** The processor script relies on finding the clustered heatmap image in the HTML generated by the visualizer to insert interpretations nearby. Significant changes to the visualizer's output structure could break the processor's ability to insert interpretations correctly.
